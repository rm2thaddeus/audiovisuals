# Expertise Assessment Framework

## Purpose
Systematically identify user expertise gaps to enable targeted AI assistance that bridges knowledge while preserving user agency.

## Assessment Strategy

### 1. Technical Foundation Assessment
**Programming Languages:**
- What languages do you use regularly? (Python, JavaScript, Go, etc.)
- Which concepts challenge you? (async/await, closures, memory management, etc.)
- How do you typically learn new languages? (tutorials, documentation, examples)

**Frameworks & Tools:**
- What frameworks are you comfortable with? (React, FastAPI, Django, etc.)
- Which development tools do you use? (editors, debuggers, profilers)
- What's your testing experience? (unit tests, integration tests, TDD)

### 2. AI Tools Proficiency
**Prompting Experience:**
- How do you typically structure prompts?
- What prompting techniques have you tried?
- Where do you struggle with AI interactions?

**MCP Server Knowledge:**
- Which MCP servers are you familiar with?
- What integration patterns have you used?
- Where do you need guidance?

### 3. Project Context Gaps
**Domain Knowledge:**
- What's your experience in the project domain?
- Which concepts are unfamiliar?
- What industry patterns should you know?

**Architecture Understanding:**
- How do you approach system design?
- What scalability considerations do you make?
- How do you handle security requirements?

### 4. Learning Preferences
**Information Processing:**
- Do you prefer step-by-step or conceptual overviews?
- How do you like to receive code explanations?
- What's your preferred error handling approach?

**Collaboration Style:**
- Do you want AI to be proactive or reactive?
- How do you prefer to make technical decisions?
- What feedback frequency works best?

## Gap Identification Questions

### For Programming Gaps:
1. "When you encounter a new programming concept, how do you typically approach learning it?"
2. "What programming tasks do you find most challenging or time-consuming?"
3. "How do you typically debug issues in your code?"

### For AI Tool Gaps:
1. "What's your experience with prompting AI assistants for coding tasks?"
2. "Have you used MCP servers or similar AI tool integrations?"
3. "What aspects of AI-assisted development feel most unfamiliar?"

### For Project Context Gaps:
1. "What's your experience with [project domain]?"
2. "Which architectural patterns or concepts are you unfamiliar with?"
3. "What scalability or security considerations should we address?"

## Progressive Disclosure System

### Level 1: Basic Assessment (5-10 minutes)
- Core programming language comfort
- Primary tool familiarity
- Basic prompting experience
- Project domain knowledge

### Level 2: Detailed Assessment (10-15 minutes)
- Specific technical gaps
- Learning preferences
- Collaboration style
- Priority learning areas

### Level 3: Deep Assessment (15-20 minutes)
- Advanced technical concepts
- Complex integration patterns
- Architectural decision-making
- Long-term learning goals

## AI Adaptation Based on Gaps

### High Technical Gaps:
- Provide step-by-step explanations
- Include inline comments and documentation
- Offer multiple implementation approaches
- Explain trade-offs and alternatives

### Medium Technical Gaps:
- Focus on key concepts and patterns
- Provide reference links for deeper learning
- Suggest best practices and conventions
- Explain reasoning behind recommendations

### Low Technical Gaps:
- Provide concise, targeted assistance
- Focus on optimization and advanced patterns
- Suggest architectural improvements
- Enable independent exploration

## Success Metrics
- [ ] User can articulate their main project intent clearly
- [ ] Specific expertise gaps are identified and documented
- [ ] AI assistance strategy is tailored to user needs
- [ ] Learning path is established for priority gaps
- [ ] Collaboration preferences are established
